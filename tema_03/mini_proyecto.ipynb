{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 20:42:47.990241: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:\n",
      "2024-11-24 20:42:47.990260: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from time import perf_counter\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 1\n",
    "\n",
    "def get_datos():\n",
    "    datos=[[  2.01666708 ,  56.18031474], [  4.79734083 ,  47.18848199], [  9.23784581 ,  57.68974048], [ 14.11529384 ,  43.70348368],\n",
    "         [ 14.92688637 ,  59.10244323], [ 17.34408196 ,  65.96080804], [ 17.62435324 ,  45.74334603], [ 22.41875021 ,  13.575581  ],\n",
    "         [ 25.3139145  ,  68.43756969], [ 34.85886672 , 147.15375307], [ 38.87476262 ,  25.39687009], [ 42.01380169 ,  74.39010069],\n",
    "         [ 46.63551059 ,  98.93395801], [ 49.58578273 , 116.07013679], [ 50.18371003 , 138.55546747], [ 52.06630172 , 139.36601894],\n",
    "         [ 54.68810274 , 150.09622546], [ 57.13046193 , 156.14375739], [ 66.35609935 , 119.75844452], [ 69.05499042 , 139.08155228],\n",
    "         [ 69.51252436 , 128.72247348], [ 69.83788756 , 152.65110462], [ 79.76649746 , 148.23106977], [ 81.83730552 , 137.86314926],\n",
    "         [ 87.09879038 , 217.28932067], [ 89.00469759 , 168.64994509], [ 93.17139213 , 163.10598352], [ 93.66070686 , 200.47638924],\n",
    "         [ 94.1944751  , 150.44019156], [ 97.36920633 , 173.2055957 ]]\n",
    "\n",
    "    return datos\n",
    "\n",
    "\n",
    "datos=np.array(get_datos())\n",
    "\n",
    "x_entrenamiento=datos[:,0].reshape(-1,1)\n",
    "y_entrenamiento=datos[:,1].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 20:42:49.367162: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:\n",
      "2024-11-24 20:42:49.367246: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:\n",
      "2024-11-24 20:42:49.367305: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:\n",
      "2024-11-24 20:42:49.367362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:\n",
      "2024-11-24 20:42:49.402472: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:\n",
      "2024-11-24 20:42:49.402557: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:\n",
      "2024-11-24 20:42:49.402568: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-11-24 20:42:49.402932: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Paso 2\n",
    "def compile_fit(capas,x,y,epochs):\n",
    "    np.random.seed(5)\n",
    "    tf.random.set_seed(5)\n",
    "    random.seed(5)  \n",
    "    \n",
    "    model=Sequential()\n",
    "    for index,neuronas_capa in enumerate(capas):\n",
    "        if (index==0):\n",
    "            model.add(Dense(neuronas_capa, activation='relu',input_dim=x.shape[1]))\n",
    "        elif (index==len(capas)-1):\n",
    "            model.add(Dense(neuronas_capa, activation='linear'))\n",
    "        else:\n",
    "            model.add(Dense(neuronas_capa, activation='relu'))\n",
    "    model.compile(loss='mean_squared_error')\n",
    "    \n",
    "    history=model.fit(x, y,epochs=epochs,verbose=False) \n",
    "\n",
    "    return model,history\n",
    "\n",
    "capas=[5,10,3,10,1]\n",
    "epochs=5000\n",
    "model,history=compile_fit(capas,x_entrenamiento,y_entrenamiento,epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       x    y_true    y_pred     error\n",
      "--------  --------  --------  --------\n",
      " 2.01667   56.1803   36.5675  384.663\n",
      " 4.79734   47.1885   40.9242   39.2418\n",
      " 9.23785   57.6897   47.8814   96.2036\n",
      "14.1153    43.7035   55.5232  139.707\n"
     ]
    }
   ],
   "source": [
    "#Paso 3\n",
    "\n",
    "x_paso3=np.array([ 2.01666708, 4.79734083, 9.23784581, 14.11529384 ]).reshape(-1,1)\n",
    "y_true_paso3=np.array([ 56.18031474, 47.18848199, 57.68974048, 43.70348368]).reshape(-1,1)\n",
    "y_pred_paso3=model.predict(x_paso3,verbose=False)\n",
    "y_error_paso3=(y_true_paso3-y_pred_paso3)**2\n",
    "\n",
    "resultado_paso3=np.column_stack((x_paso3,y_true_paso3,y_pred_paso3,y_error_paso3))\n",
    "\n",
    "print(tabulate(resultado_paso3,headers=[\"x\",\"y_true\",\"y_pred\",\"error\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE)=164.954\n"
     ]
    }
   ],
   "source": [
    "#Paso 4\n",
    "\n",
    "mse=np.sum(y_error_paso3)/np.size(y_error_paso3)\n",
    "print(f\"Mean Squared Error (MSE)={mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente de determinación o R²=-3.740\n"
     ]
    }
   ],
   "source": [
    "#Paso 5\n",
    "media_valores_verdaderos=np.sum(y_true_paso3)/np.size(y_true_paso3)\n",
    "suma_errores=np.sum((y_true_paso3-y_pred_paso3)**2)\n",
    "suma_valores_media=np.sum((y_true_paso3-media_valores_verdaderos)**2)\n",
    "coeficiente_determinacion=1-(suma_errores/suma_valores_media)\n",
    "\n",
    "print(f\"Coeficiente de determinación o R²={coeficiente_determinacion:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE)=718.298\n",
      "Coeficiente de determinación o R²=0.748\n"
     ]
    }
   ],
   "source": [
    "#Paso 6\n",
    "def get_mse(y_true,y_pred):\n",
    "    y_error=(y_true-y_pred)**2\n",
    "    mse=np.sum(y_error)/np.size(y_error)\n",
    "\n",
    "    return mse\n",
    "\n",
    "def get_coeficiente_determinacion(y_true,y_pred):\n",
    "    media_valores_verdaderos=np.sum(y_true)/np.size(y_true)\n",
    "    suma_errores=np.sum((y_true-y_pred)**2)\n",
    "    suma_valores_media=np.sum((y_true-media_valores_verdaderos)**2)\n",
    "    coeficiente_determinacion=1-(suma_errores/suma_valores_media)\n",
    "\n",
    "    return coeficiente_determinacion\n",
    "\n",
    "\n",
    "def get_metricas_modelo(model,x,y_true):\n",
    "    y_pred=model.predict(x,verbose=False)\n",
    "    mse=get_mse(y_true,y_pred)\n",
    "    coeficiente_determinacion=get_coeficiente_determinacion(y_true,y_pred)\n",
    "\n",
    "    return mse,coeficiente_determinacion\n",
    "\n",
    "mse,coeficiente_determinacion=get_metricas_modelo(model,x_entrenamiento,y_entrenamiento)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE)={mse:.3f}\")\n",
    "print(f\"Coeficiente de determinación o R²={coeficiente_determinacion:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE)=539.139\n",
      "Coeficiente de determinación o R²=0.811\n"
     ]
    }
   ],
   "source": [
    "#Paso 7\n",
    "capas=[20,40,80,40,20,1]\n",
    "model,history=compile_fit(capas,x_entrenamiento,y_entrenamiento,epochs)\n",
    "mse,coeficiente_determinacion=get_metricas_modelo(model,x_entrenamiento,y_entrenamiento)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE)={mse:.3f}\")\n",
    "print(f\"Coeficiente de determinación o R²={coeficiente_determinacion:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE)=182.719\n",
      "Coeficiente de determinación o R²=0.936\n"
     ]
    }
   ],
   "source": [
    "#Paso 8\n",
    "\n",
    "def compile_fit(capas,x,y,epochs,activation):\n",
    "    np.random.seed(5)\n",
    "    tf.random.set_seed(5)\n",
    "    random.seed(5)  \n",
    "    \n",
    "    model=Sequential()\n",
    "    for index,neuronas_capa in enumerate(capas):\n",
    "        if (index==0):\n",
    "            model.add(Dense(neuronas_capa, activation=activation,input_dim=x.shape[1]))\n",
    "        elif (index==len(capas)-1):\n",
    "            model.add(Dense(neuronas_capa, activation='linear'))\n",
    "        else:\n",
    "            model.add(Dense(neuronas_capa, activation=activation))\n",
    "    model.compile(loss='mean_squared_error')\n",
    "    \n",
    "    history=model.fit(x, y,epochs=epochs,verbose=False) \n",
    "\n",
    "    return model,history\n",
    "\n",
    "\n",
    "\n",
    "capas=[20,40,80,40,20,1]\n",
    "model,history=compile_fit(capas,x_entrenamiento,y_entrenamiento,epochs,\"selu\")\n",
    "mse,coeficiente_determinacion=get_metricas_modelo(model,x_entrenamiento,y_entrenamiento)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE)={mse:.3f}\")\n",
    "print(f\"Coeficiente de determinación o R²={coeficiente_determinacion:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f15bb86b490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f15bac4d240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Nombre  Capas                               Épocas  Activación         MSE           R²           Tiempo\n",
      "     Red                                                                                     Entrenamiento\n",
      "--------  --------------------------------  --------  ------------  --------  -----------  ---------------\n",
      "       1  [5, 10, 3, 10, 1]                     5000  relu           718.298   0.748316            3.7071\n",
      "       2  [5, 10, 3, 10, 1]                     5000  selu           717.653   0.748541            3.71069\n",
      "       3  [5, 10, 3, 10, 1]                     5000  tanh          5941.53   -1.08185             3.69128\n",
      "       4  [20, 40, 80, 40, 20, 1]               5000  relu           539.139   0.811091            4.11419\n",
      "       5  [20, 40, 80, 40, 20, 1]               5000  selu           182.719   0.935977            3.94965\n",
      "       6  [20, 40, 80, 40, 20, 1]               5000  tanh          2883.8    -0.0104537           4.06745\n",
      "       7  [20, 40, 80, 160, 80, 40, 20, 1]      5000  relu           396.495   0.861072            4.74582\n",
      "       8  [20, 40, 80, 160, 80, 40, 20, 1]      5000  selu           172.284   0.939634            4.63003\n",
      "       9  [20, 40, 80, 160, 80, 40, 20, 1]      5000  tanh          2866.53   -0.00440395          4.45341\n"
     ]
    }
   ],
   "source": [
    "#Paso 9\n",
    "\n",
    "def compile_fit(capas,x,y,epochs,activation):\n",
    "    np.random.seed(5)\n",
    "    tf.random.set_seed(5)\n",
    "    random.seed(5)  \n",
    "    \n",
    "    model=Sequential()\n",
    "    for index,neuronas_capa in enumerate(capas):\n",
    "        if (index==0):\n",
    "            model.add(Dense(neuronas_capa, activation=activation,input_dim=x.shape[1]))\n",
    "        elif (index==len(capas)-1):\n",
    "            model.add(Dense(neuronas_capa, activation='linear'))\n",
    "        else:\n",
    "            model.add(Dense(neuronas_capa, activation=activation))\n",
    "    model.compile(loss='mean_squared_error')\n",
    "    \n",
    "    tiempo_entrenamiento = perf_counter()\n",
    "    history=model.fit(x, y,epochs=epochs,verbose=False) \n",
    "    tiempo_entrenamiento=perf_counter()-tiempo_entrenamiento\n",
    "\n",
    "    return model,history,tiempo_entrenamiento\n",
    "\n",
    "redes_neuronales=[\n",
    "    [[5,10,3,10,1],\"relu\"],\n",
    "    [[5,10,3,10,1],\"selu\"],\n",
    "    [[5,10,3,10,1],\"tanh\"],    \n",
    "    [[20,40,80,40,20,1],\"relu\"],\n",
    "    [[20,40,80,40,20,1],\"selu\"],\n",
    "    [[20,40,80,40,20,1],\"tanh\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"relu\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"selu\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"tanh\"]                           \n",
    "]\n",
    "epochs=5000\n",
    "\n",
    "resultados=[]\n",
    "\n",
    "for index,(capas,activacion) in enumerate(redes_neuronales):\n",
    "\n",
    "    model,history,tiempo_entrenamiento=compile_fit(capas,x_entrenamiento,y_entrenamiento,epochs,activacion)\n",
    "    mse_entrenamiento,coeficiente_determinacion_entrenamiento=get_metricas_modelo(model,x_entrenamiento,y_entrenamiento)\n",
    "\n",
    "    resultado=[str(index+1),str(capas),epochs,activacion,mse_entrenamiento,coeficiente_determinacion_entrenamiento,tiempo_entrenamiento]\n",
    "    resultados.append(resultado)\n",
    "\n",
    "print(tabulate(resultados,headers=[\"Nombre\\nRed\",\"Capas\",\"Épocas\",\"Activación\",\"MSE\",\"R²\",\"Tiempo\\nEntrenamiento\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nombre  Capas                               Épocas  Activación                MSE               R²           Tiempo           MSE            R²\n",
      "     Red                                                              Entrenamiento    Entrenamiento    Entrenamiento    Validación    Validación\n",
      "--------  --------------------------------  --------  ------------  ---------------  ---------------  ---------------  ------------  ------------\n",
      "       1  [5, 10, 3, 10, 1]                     5000  relu                  718.298       0.748316            3.68986       862.985     0.567329\n",
      "       2  [5, 10, 3, 10, 1]                     5000  selu                  717.653       0.748541            4.02641       798.624     0.599598\n",
      "       3  [5, 10, 3, 10, 1]                     5000  tanh                 5941.53       -1.08185             3.77017      3753.3      -0.881774\n",
      "       4  [20, 40, 80, 40, 20, 1]               5000  relu                  539.139       0.811091            4.01741       846.146     0.575772\n",
      "       5  [20, 40, 80, 40, 20, 1]               5000  selu                  182.719       0.935977            4.00112      1051.1       0.473016\n",
      "       6  [20, 40, 80, 40, 20, 1]               5000  tanh                 2883.8        -0.0104537           3.91675      2061.24     -0.0334357\n",
      "       7  [20, 40, 80, 160, 80, 40, 20, 1]      5000  relu                  396.495       0.861072            4.42832       990.747     0.503274\n",
      "       8  [20, 40, 80, 160, 80, 40, 20, 1]      5000  selu                  172.284       0.939634            4.58907       898.996     0.549275\n",
      "       9  [20, 40, 80, 160, 80, 40, 20, 1]      5000  tanh                 2866.53       -0.00440395          4.41973      2096.22     -0.0509744\n"
     ]
    }
   ],
   "source": [
    "#Paso 10\n",
    "def get_datos_validacion():\n",
    "    datos_validacion=[[  1.22140488 , 59.35315077] , [  2.42834632 ,  3.50613409] , [  4.27529991 , 70.39938914] ,\n",
    "        [ 14.44651349 , 50.0606769 ] , [ 16.10795855 , 81.08562061] , [ 16.75024181 , 33.95365822] ,\n",
    "        [ 26.80487149 , 47.1495392 ] , [ 28.81517859 ,106.34919698] , [ 48.56698654 ,120.25398606] ,\n",
    "        [ 52.08015067 ,116.7993955 ] , [ 53.30646055 ,131.30936472] , [ 55.09968806 ,131.34281752] ,\n",
    "        [ 60.39615207 , 97.77483743] , [ 73.52487026 , 92.30645543] , [ 76.2771471  ,109.9995226 ] ,\n",
    "        [ 84.56808303 ,120.60657657] , [ 89.2700557  ,117.3687155 ] , [ 91.03720679 ,159.47376137] ,\n",
    "        [ 93.53406333 ,166.44439331] , [ 94.78103495 ,180.66942656]]\n",
    "    return datos_validacion\n",
    "\n",
    "datos_validacion=np.array(get_datos_validacion())\n",
    "x_validacion=datos_validacion[:,0].reshape(-1,1)\n",
    "y_validacion=datos_validacion[:,1].reshape(-1,1)\n",
    "\n",
    "\n",
    "resultados=[]\n",
    "\n",
    "for index,(capas,activacion) in enumerate(redes_neuronales):\n",
    "\n",
    "    model,history,tiempo_entrenamiento=compile_fit(capas,x_entrenamiento,y_entrenamiento,epochs,activacion)\n",
    "    mse_entrenamiento,coeficiente_determinacion_entrenamiento=get_metricas_modelo(model,x_entrenamiento,y_entrenamiento)\n",
    "    mse_validacion,coeficiente_determinacion_validacion=get_metricas_modelo(model,x_validacion,y_validacion)   \n",
    "\n",
    "    resultado=[str(index+1),str(capas),epochs,activacion,mse_entrenamiento,coeficiente_determinacion_entrenamiento,tiempo_entrenamiento,mse_validacion,coeficiente_determinacion_validacion]\n",
    "    resultados.append(resultado)\n",
    "\n",
    "print(tabulate(resultados,headers=[\"Nombre\\nRed\",\"Capas\",\"Épocas\",\"Activación\",\"MSE\\nEntrenamiento\",\"R²\\nEntrenamiento\",\"Tiempo\\nEntrenamiento\",\"MSE\\nValidación\",\"R²\\nValidación\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nombre  Capas                               Épocas  Activación                MSE               R²           Tiempo           MSE            R²\n",
      "     Red                                                              Entrenamiento    Entrenamiento    Entrenamiento    Validación    Validación\n",
      "--------  --------------------------------  --------  ------------  ---------------  ---------------  ---------------  ------------  ------------\n",
      "       1  [5, 10, 3, 10, 1]                     5000  relu                  718.298       0.748316            3.76294       862.985     0.567329\n",
      "       2  [5, 10, 3, 10, 1]                     5000  selu                  717.653       0.748541            3.75726       798.624     0.599598\n",
      "       3  [5, 10, 3, 10, 1]                     5000  tanh                 5941.53       -1.08185             3.72517      3753.3      -0.881774\n",
      "       4  [20, 40, 80, 40, 20, 1]               5000  relu                  539.139       0.811091            3.92204       846.146     0.575772\n",
      "       5  [20, 40, 80, 40, 20, 1]               5000  selu                  182.719       0.935977            3.96438      1051.1       0.473016\n",
      "       6  [20, 40, 80, 40, 20, 1]               5000  tanh                 2883.8        -0.0104537           3.85926      2061.24     -0.0334357\n",
      "       7  [20, 40, 80, 160, 80, 40, 20, 1]      5000  relu                  396.495       0.861072            4.43906       990.747     0.503274\n",
      "       8  [20, 40, 80, 160, 80, 40, 20, 1]      5000  selu                  172.284       0.939634            4.85747       898.996     0.549275\n",
      "       9  [20, 40, 80, 160, 80, 40, 20, 1]      5000  tanh                 2866.53       -0.00440395          4.46272      2096.22     -0.0509744\n"
     ]
    }
   ],
   "source": [
    "#Paso 11\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "from tabulate import tabulate\n",
    "from time import perf_counter\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "def get_datos():\n",
    "    datos=[[  2.01666708 ,  56.18031474], [  4.79734083 ,  47.18848199], [  9.23784581 ,  57.68974048], [ 14.11529384 ,  43.70348368],\n",
    "         [ 14.92688637 ,  59.10244323], [ 17.34408196 ,  65.96080804], [ 17.62435324 ,  45.74334603], [ 22.41875021 ,  13.575581  ],\n",
    "         [ 25.3139145  ,  68.43756969], [ 34.85886672 , 147.15375307], [ 38.87476262 ,  25.39687009], [ 42.01380169 ,  74.39010069],\n",
    "         [ 46.63551059 ,  98.93395801], [ 49.58578273 , 116.07013679], [ 50.18371003 , 138.55546747], [ 52.06630172 , 139.36601894],\n",
    "         [ 54.68810274 , 150.09622546], [ 57.13046193 , 156.14375739], [ 66.35609935 , 119.75844452], [ 69.05499042 , 139.08155228],\n",
    "         [ 69.51252436 , 128.72247348], [ 69.83788756 , 152.65110462], [ 79.76649746 , 148.23106977], [ 81.83730552 , 137.86314926],\n",
    "         [ 87.09879038 , 217.28932067], [ 89.00469759 , 168.64994509], [ 93.17139213 , 163.10598352], [ 93.66070686 , 200.47638924],\n",
    "         [ 94.1944751  , 150.44019156], [ 97.36920633 , 173.2055957 ]]\n",
    "\n",
    "    return datos\n",
    "\n",
    "def get_datos_validacion():\n",
    "    datos_validacion=[[  1.22140488 , 59.35315077] , [  2.42834632 ,  3.50613409] , [  4.27529991 , 70.39938914] ,\n",
    "        [ 14.44651349 , 50.0606769 ] , [ 16.10795855 , 81.08562061] , [ 16.75024181 , 33.95365822] ,\n",
    "        [ 26.80487149 , 47.1495392 ] , [ 28.81517859 ,106.34919698] , [ 48.56698654 ,120.25398606] ,\n",
    "        [ 52.08015067 ,116.7993955 ] , [ 53.30646055 ,131.30936472] , [ 55.09968806 ,131.34281752] ,\n",
    "        [ 60.39615207 , 97.77483743] , [ 73.52487026 , 92.30645543] , [ 76.2771471  ,109.9995226 ] ,\n",
    "        [ 84.56808303 ,120.60657657] , [ 89.2700557  ,117.3687155 ] , [ 91.03720679 ,159.47376137] ,\n",
    "        [ 93.53406333 ,166.44439331] , [ 94.78103495 ,180.66942656]]\n",
    "    return datos_validacion\n",
    "\n",
    "def compile_fit(capas,x,y,epochs,activation):\n",
    "    np.random.seed(5)\n",
    "    tf.random.set_seed(5)\n",
    "    random.seed(5)  \n",
    "    \n",
    "    model=Sequential()\n",
    "    for index,neuronas_capa in enumerate(capas):\n",
    "        if (index==0):\n",
    "            model.add(Dense(neuronas_capa, activation=activation,input_dim=x.shape[1]))\n",
    "        elif (index==len(capas)-1):\n",
    "            model.add(Dense(neuronas_capa, activation='linear'))\n",
    "        else:\n",
    "            model.add(Dense(neuronas_capa, activation=activation))\n",
    "    model.compile(loss='mean_squared_error')\n",
    "    \n",
    "    tiempo_entrenamiento = perf_counter()\n",
    "    history=model.fit(x, y,epochs=epochs,verbose=False) \n",
    "    tiempo_entrenamiento=perf_counter()-tiempo_entrenamiento\n",
    "\n",
    "    return model,history,tiempo_entrenamiento\n",
    "\n",
    "def get_mse(y_true,y_pred):\n",
    "    y_error=(y_true-y_pred)**2\n",
    "    mse=np.sum(y_error)/np.size(y_error)\n",
    "\n",
    "    return mse\n",
    "\n",
    "def get_coeficiente_determinacion(y_true,y_pred):\n",
    "    media_valores_verdaderos=np.sum(y_true)/np.size(y_true)\n",
    "    suma_errores=np.sum((y_true-y_pred)**2)\n",
    "    suma_valores_media=np.sum((y_true-media_valores_verdaderos)**2)\n",
    "    coeficiente_determinacion=1-(suma_errores/suma_valores_media)\n",
    "\n",
    "    return coeficiente_determinacion\n",
    "\n",
    "\n",
    "def get_metricas_modelo(model,x,y_true):\n",
    "    y_pred=model.predict(x,verbose=False)\n",
    "    mse=get_mse(y_true,y_pred)\n",
    "    coeficiente_determinacion=get_coeficiente_determinacion(y_true,y_pred)\n",
    "\n",
    "    return mse,coeficiente_determinacion\n",
    "\n",
    "\n",
    "datos=np.array(get_datos())\n",
    "x_entrenamiento=datos[:,0].reshape(-1,1)\n",
    "y_entrenamiento=datos[:,1].reshape(-1,1)\n",
    "\n",
    "datos_validacion=np.array(get_datos_validacion())\n",
    "x_validacion=datos_validacion[:,0].reshape(-1,1)\n",
    "y_validacion=datos_validacion[:,1].reshape(-1,1)\n",
    "\n",
    "\n",
    "redes_neuronales=[\n",
    "    [[5,10,3,10,1],\"relu\"],\n",
    "    [[5,10,3,10,1],\"selu\"],\n",
    "    [[5,10,3,10,1],\"tanh\"],    \n",
    "    [[20,40,80,40,20,1],\"relu\"],\n",
    "    [[20,40,80,40,20,1],\"selu\"],\n",
    "    [[20,40,80,40,20,1],\"tanh\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"relu\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"selu\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"tanh\"]                           \n",
    "]\n",
    "epochs=5000\n",
    "\n",
    "resultados=[]\n",
    "\n",
    "for index,(capas,activacion) in enumerate(redes_neuronales):\n",
    "\n",
    "    model,history,tiempo_entrenamiento=compile_fit(capas,x_entrenamiento,y_entrenamiento,epochs,activacion)\n",
    "    mse_entrenamiento,coeficiente_determinacion_entrenamiento=get_metricas_modelo(model,x_entrenamiento,y_entrenamiento)\n",
    "    mse_validacion,coeficiente_determinacion_validacion=get_metricas_modelo(model,x_validacion,y_validacion)   \n",
    "\n",
    "    resultado=[str(index+1),str(capas),epochs,activacion,mse_entrenamiento,coeficiente_determinacion_entrenamiento,tiempo_entrenamiento,mse_validacion,coeficiente_determinacion_validacion]\n",
    "    resultados.append(resultado)  \n",
    "\n",
    "print(tabulate(resultados,headers=[\"Nombre\\nRed\",\"Capas\",\"Épocas\",\"Activación\",\"MSE\\nEntrenamiento\",\"R²\\nEntrenamiento\",\"Tiempo\\nEntrenamiento\",\"MSE\\nValidación\",\"R²\\nValidación\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/logongas/Documentos/desarrollo/python_default_env/.venv/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nombre  Capas                               Épocas  Activación                MSE               R²           Tiempo           MSE            R²\n",
      "     Red                                                              Entrenamiento    Entrenamiento    Entrenamiento    Validación    Validación\n",
      "--------  --------------------------------  --------  ------------  ---------------  ---------------  ---------------  ------------  ------------\n",
      "       1  [5, 10, 3, 10, 1]                     1000  relu                 14.2731       0.835702             6.2579        15.8263     0.784189\n",
      "       2  [5, 10, 3, 10, 1]                     1000  selu                 15.7082       0.819183             6.14064       15.069      0.794514\n",
      "       3  [5, 10, 3, 10, 1]                     1000  tanh                 27.0317       0.688839             6.11757       30.2255     0.587836\n",
      "       4  [20, 40, 80, 40, 20, 1]               1000  relu                  3.91011      0.954991             6.60896       13.852      0.811111\n",
      "       5  [20, 40, 80, 40, 20, 1]               1000  selu                  2.92343      0.966348             6.81924       16.3968     0.776409\n",
      "       6  [20, 40, 80, 40, 20, 1]               1000  tanh                 86.8738      -4.41074e-06          6.63918       75.0985    -0.0240648\n",
      "       7  [20, 40, 80, 160, 80, 40, 20, 1]      1000  relu                  7.80517      0.910155             8.41362       22.2737     0.696269\n",
      "       8  [20, 40, 80, 160, 80, 40, 20, 1]      1000  selu                  1.24069      0.985718             8.41598       15.1869     0.792908\n",
      "       9  [20, 40, 80, 160, 80, 40, 20, 1]      1000  tanh                 86.8738      -4.41074e-06          8.07197       75.0985    -0.0240648\n"
     ]
    }
   ],
   "source": [
    "def get_datos():\n",
    "    boston=fetch_openml(name=\"boston\",version=1)   \n",
    "    x=np.array(boston.data).astype(np.float32)\n",
    "    y=np.array(boston.target).astype(np.float32)\n",
    "\n",
    "    x_entrenamiento, x_validacion, y_entrenamiento, y_validacion = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return x_entrenamiento, y_entrenamiento.reshape(-1,1), x_validacion, y_validacion.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "x_entrenamiento, y_entrenamiento, x_validacion, y_validacion=get_datos()\n",
    "\n",
    "redes_neuronales=[\n",
    "    [[5,10,3,10,1],\"relu\"],\n",
    "    [[5,10,3,10,1],\"selu\"],\n",
    "    [[5,10,3,10,1],\"tanh\"],    \n",
    "    [[20,40,80,40,20,1],\"relu\"],\n",
    "    [[20,40,80,40,20,1],\"selu\"],\n",
    "    [[20,40,80,40,20,1],\"tanh\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"relu\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"selu\"],\n",
    "    [[20,40,80,160,80,40,20,1],\"tanh\"]                               \n",
    "]                           \n",
    "\n",
    "epochs=1000\n",
    "\n",
    "resultados=[]\n",
    "\n",
    "\n",
    "\n",
    "for index,(capas,activacion) in enumerate(redes_neuronales):\n",
    "\n",
    "    model,history,tiempo_entrenamiento=compile_fit(capas,x_entrenamiento,y_entrenamiento,epochs,activacion)\n",
    "    mse_entrenamiento,coeficiente_determinacion_entrenamiento=get_metricas_modelo(model,x_entrenamiento,y_entrenamiento)\n",
    "    mse_validacion,coeficiente_determinacion_validacion=get_metricas_modelo(model,x_validacion,y_validacion)   \n",
    "\n",
    "    resultado=[str(index+1),str(capas),epochs,activacion,mse_entrenamiento,coeficiente_determinacion_entrenamiento,tiempo_entrenamiento,mse_validacion,coeficiente_determinacion_validacion]\n",
    "    resultados.append(resultado)  \n",
    "\n",
    "print(tabulate(resultados,headers=[\"Nombre\\nRed\",\"Capas\",\"Épocas\",\"Activación\",\"MSE\\nEntrenamiento\",\"R²\\nEntrenamiento\",\"Tiempo\\nEntrenamiento\",\"MSE\\nValidación\",\"R²\\nValidación\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
